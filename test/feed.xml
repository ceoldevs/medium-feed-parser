<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Meherchaitanya on Medium]]></title>
        <description><![CDATA[Stories by Meherchaitanya on Medium]]></description>
        <link>https://medium.com/@smc181002?source=rss-ee5d12b9cb1a------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*xIRCwr3HVJx0aXc38HcgKw.png</url>
            <title>Stories by Meherchaitanya on Medium</title>
            <link>https://medium.com/@smc181002?source=rss-ee5d12b9cb1a------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 23 May 2023 08:21:54 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@smc181002/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[How Loops in Network can be solved with STP (Using 7 switches in topology)]]></title>
            <link>https://smc181002.medium.com/how-loops-in-network-can-be-solved-with-stp-using-7-switches-in-topology-c54ae3b040cb?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/c54ae3b040cb</guid>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[stp-network]]></category>
            <category><![CDATA[spanning-tree]]></category>
            <category><![CDATA[network]]></category>
            <category><![CDATA[network-switches]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Mon, 02 Jan 2023 16:05:25 GMT</pubDate>
            <atom:updated>2023-01-02T16:05:25.922Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HLKv3OBd-ls2O6qzOo3oSw.png" /></figure><h3>TL;DR</h3><p>Looping is a problem caused by the configuration topology in either the data link or the network layer. In the Data link layer, this is caused when switches are connected in a loop leading to multiple paths to reach a destination. Multiple paths can be caused either due to misconfiguration or to provide fault tolerance through redundancy. To eliminate loops, STP is a protocol that switches use. This article focuses on using a bigger topology to properly understand STP packets.</p><h3>How switch learns the network</h3><p>The switch stores the MAC addresses learnt from the packets passing through the ports and in the forwarding table (also referred to as the CAM table). When a packet comes from a system, the source MAC address is stored in the forwarding table and, therefore, learns the ports associated with the MAC addresses.</p><p>But if there are multiple ports that the MAC address is associated with, a standard switch would send the packets to both of its ports. In this process, if we have other switches connected in these ports, the</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZJN7dazB5BpuWHw6VRSsVA.gif" /><figcaption>Looping in the switch that occurs when trying to send a packet from one switch to another in presence of a loop</figcaption></figure><p>From the above graphic, it can be seen that when a packet is supposed to reach a device in <strong>switch 2</strong> from a device in <strong>switch 1</strong>, <strong>switch 1</strong> has two ways to contact switch 2. So the switch sends the packet through both interfaces (ports). Once the packet reaches <strong>switch 3</strong>, the packet can now go back to <strong>switch 1</strong> and to <strong>switch 2</strong> leading to a packet sent back to<strong> switch 1</strong>.</p><p>With this, we reached our initial state and <strong>switch 1</strong> sends the packet back to <strong>switch 3</strong> leading to a loop in packets. This will cause inefficient usage of bandwidth in the network and at some point, the switch will be overloaded and shut down.</p><p>In the network layer, due to the TTL parameter, the routers will drop the packets after some time but there is no similar feature in the link layer and the data packet will be there in the network until restarted or the link is broken.</p><p>One simple solution is to just remove these links to not have a loop. But this solution will remove the network fault tolerance feature for which we have multiple links. To solve this, there exists a protocol called Spanning tree Protocol (STP). With the help of this protocol, the graph network of switches will be converted into an acyclic graph.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Oxodop16y4KZleaCER2wLg.png" /></figure><h3>STP protocol with 7 switches</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FxptIcvs_gHXR9NmHQA-oQ.png" /></figure><p>From the above topology of switches, if the switch 1 is chosen the root Node and assume each of the links between switches are of same cost (inversely proportional to bandwidth).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KP-AOrJm0KV0Rss2Qtvozw.png" /></figure><p>Once the root port is chosen, the ports connected to the root switch are set as <em>designated port</em>. The ports connected to the designated ports will be set as <em>root port</em> in this case meaning that this port is the connection link to the root switch with lowest cost.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CoNA27ytnHEGjHrFcIy_sA.png" /></figure><p>The same logic is extended to the neighboring nodes with root port</p><p>A loop which can be seen is that to reach <strong>root switch</strong> from <strong>switch 3</strong>, it can directly go to the <strong>switch 1</strong> or go to <strong>switch 2</strong> first and reach <strong>root switch</strong>.</p><p>For tie breaker, the path cost is first looked up and it is obvious in this case the path to be chosen is the path directly connecting to the root switch as the path cost will be double of the current path cost if the connection is maintained from <strong>switch 2</strong>.</p><p>For <strong>Switch 5</strong>, there is only one link so the <strong>switch 3</strong> port connecting to switch 5 is set as <em>designated port</em> and the port in <strong>switch 5</strong> connection is set to <em>Root port</em>.</p><p>Coming to <strong>switch 6</strong>, it has two paths, one from <strong>switch 4</strong> and another from <strong>switch 3</strong>. The tiebreaker of the path cost fails in this case as both paths has got the same cost for propagation. In this case, the switch’s the priority number is compared.</p><p>The priority numbers are written in the image under the switch name and the switch with smaller cumulative value of for the paths to be compared is low, that path is chosen.</p><p>So in this case, the <strong>switch 3</strong> port is set as <em>designated port</em> and the <strong>switch 6</strong> port connecting to switch 3 is set as <em>root port</em>. The <strong>switch 4</strong> port connecting to switch 6 will be set as <em>designated port</em> but the port of the <strong>switch 6</strong> connecting switch 4 will be a <em>blocked port</em> meaning the <strong>switch 6</strong> wont forward (ignore) any packets being sent to this port.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*q4JK-RrPXrPKnNOaNWcaHA.png" /></figure><p>Now it is the question for switch 7 and switch 8.</p><p><strong>Switch 7</strong> has two paths one from <strong>switch 4 </strong>and <strong>switch 6 </strong>and the path from <strong>switch 4</strong> is shorter path to the root switch. So the port connecting from <strong>switch 7 </strong>to switch 4 is set to <em>root port </em>and the port in <strong>switch 4</strong> connecting switch 7 is set as <em>designated port</em>.</p><p>For <strong>switch 8</strong>, similar question as switch 6 occurs as there are two paths that have equal path costs (switch 3 → switch 6 or switch 4 → switch 7). So this tie breaker fails and the next tie breaker is to compare the priority number and the cumulative priority number is leads to port connecting switch 7 from <strong>switch 8</strong> being the <em>root port</em>. So the port connecting <strong>switch 7</strong> to swith 8 is set as <em>designated port</em>. On the other hand port of <strong>switch 6</strong> connecting to switch 8 is set as <em>designated port</em> but the port of <strong>switch 8</strong> connecting to switch 6 is set as <em>blocked port</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3Dx5pjDPtyORLgDIfwwYsA.png" /></figure><p>So the final topology will be devoid of loops virtually and in case if a link breaks, the BDPU packets will stop coming to the switch and the topology will change to a new spanning tree.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7oFH0rmF51WPBahNP8Z9sA.png" /></figure><p>This summarizes the usage of Spanning tree algorithm to block the switches into entering a networking loop and an example with more switches and complex linking between switches will better explain the logic of STP. Although this algorithm is simple, this is the basic protocol being discussed and many of it’s disadvantages have been addressed by improved protocols like Rapid Spanning Tree Protocol (RSTP), Per VLAN Spanning Tree (PVST) and Rapid Per VLAN Spanning Tree (R-PVST+) that solves problems with the downtime of the links to recalculate the spanning tree and provide to allow multiple spanning trees for VLANs in a single switch.</p><h3>References &amp; other learning materials</h3><ul><li><a href="https://www.computernetworkingnotes.com/ccna-study-guide/how-switch-learns-the-mac-addresses-explained.html">How switches learn MAC addresses</a></li><li><a href="https://www.youtube.com/watch?v=eiaYS780_2U">Investigating Network Loops</a></li><li><a href="https://www.youtube.com/watch?v=VkF5RmNj1tc">Understanding Spanning Tree Protocol on Four Cisco Switches in Cisco Packet Tracer</a></li><li><a href="https://kb.netgear.com/000060475/What-is-a-network-loop">What is a network loop? | Answer | NETGEAR Support</a></li><li><a href="https://www.ciscopress.com/articles/article.asp?p=2832407&amp;seqNum=5">Varieties of Spanning Tree Protocols (3.2) &gt; STP | Cisco Press</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c54ae3b040cb" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Eliminate vendor lock-in of Serverless workloads with OpenFaaS]]></title>
            <link>https://awstip.com/eliminate-vendor-lock-in-of-serverless-workloads-with-openfaas-474807383ce1?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/474807383ce1</guid>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[openfaas]]></category>
            <category><![CDATA[vendor-lock-in]]></category>
            <category><![CDATA[serverless]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Thu, 06 Oct 2022 11:55:49 GMT</pubDate>
            <atom:updated>2022-10-15T15:17:04.492Z</atom:updated>
            <content:encoded><![CDATA[<h3>Eliminate vendor locking of Serverless workloads with OpenFaaS</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Pvv2CiL035i45PWiB7xWbQ.png" /></figure><p>OpenFaaS as the name says is open-source software that provides us with a Function as a Service. Normally the serverless platforms provided by cloud providers lead to vendor lock-in and if the cloud providers decide to change the cost, it may not align with the business budgets and may need to migrate to a more cost-effective platform migration is a costly operation both in time and resources.</p><p><strong>So what is this “Function as a Service”?</strong></p><p><strong>Function as a Service</strong> allows us to build serverless event-driven architecture applications. The term <strong>Serverless </strong>doesn&#39;t imply that there isn&#39;t a server but the user should be bothered with the underlying server. These applications are made of multiple decoupled functions that are triggered upon an event.</p><p><strong>So why should I use Serverless Functions?</strong></p><p>Serverless functions are the implementation of microservice architectures where the logic of a single application is split up into multiple services that contact each other with the help of REST APIs or gRPC calls.</p><p>Compared to a monolith, where the whole application runs on a single function, microservices solve the issue of SPOF (single point of failure).</p><p>Scaling can get heavy on the resources in monolith architectures as even for one small component of the application, the entire application needs to run separately.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Fc5L5ZkUNxFedgYPT-WUnw.png" /></figure><p><strong>So why OpenFaaS?</strong></p><p>Many clouds provide us with services that can launch and manage serverless functions. <a href="https://docs.aws.amazon.com/lambda/latest/dg/welcome.html">AWS Lambda functions</a> and <a href="https://azure.microsoft.com/en-in/products/functions/#overview">Microsoft Azure Functions</a> allow us to create functions that run based on an event trigger that can be done from various methods like webhooks, time-based triggers, or another function that calls the current function. With serverless functions, we can chain the functions, branch out and call back the previous function or run as a recursive function and any other thing a normal function can do.</p><blockquote>But writing recursive functions can lead to infinite runs if the termination case was not handled and can cause “cloud overflow” where you can get billed for running millions of functions (functions run very fast leading to a lot of function calls).</blockquote><p><strong>So why not just use the existing services?</strong></p><p>The existing services in the cloud are vendor-specific, meaning the same deployment configuration and the application code are tightly locked with the vendor who provides the resources for computing. This will lead to a scenario widely known as <strong>vendor lock-in,</strong> where moving the application from one cloud provider (the resources provider) to another will incur a cost in changing the application to work in the new environment and the resources required to change the environment.</p><p>OpenFaaS is different from the existing cloud serverless platform providers as OpenFaaS works on software called Kubernetes which is called a cloud-native platform for computing. Kubernetes is a container orchestration tool that creates and manages container resources over machines. The main advantage of Kubernetes is its portability and being open source which leads to the adaptation of managed Kubernetes cluster creation in the major clouds that lead to easy deployment of the cluster in all the clouds. This also means if any application requires to be transferred to another cloud, there is no requirement for change in the application code. This helps in breaking the vendor lock-in that is being faced in the cloud environments.</p><p>So why do we fall into vendor lock-in? We fall into vendor lock-in because of the use To simply explain this scenario, Apple products have a large user base and are rumoured to have surpassed the number of users of iPhones to the Andriod users. The reason why apple products are brought extensively, especially mobile phones is because of the ecosystem that comes with the daily accessories like AirPods and Apple Watch and productivity devices mac books and iPads that simply integrate with each other.</p><p>Although this is not the exact case in the cloud, deploying the application on the provided serverless environments is much cheaper than spinning up a Kubernetes cluster. This happens because each unit of the underlying compute resource is shared between thousands of users at a time and payment is done for each unit used. But with Kubernetes, the cluster requires the launch of instances that should be managed by us and the infrastructure is not shared leading to higher prices compared to what the clouds offer in their environments. To know more about kubernetes, <strong>follow me</strong> as my next article will be about understanding kubernetes.</p><p>The requirement of using OpenFaaS or using the natively provided serverless platforms by each cloud comes down to deciding whether to <strong>pay more money in order to break the vendor lock-in and not rely on the services</strong> or to <strong>pay less money and lose the freedom of changing the vendor with no cost</strong>. This decision needs to be made by keeping the company’s compliance and budget in the decision-making process.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=474807383ce1" width="1" height="1" alt=""><hr><p><a href="https://awstip.com/eliminate-vendor-lock-in-of-serverless-workloads-with-openfaas-474807383ce1">Eliminate vendor lock-in of Serverless workloads with OpenFaaS</a> was originally published in <a href="https://awstip.com">AWS Tip</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating Terraform module for AWS VPC creation]]></title>
            <link>https://smc181002.medium.com/creating-terraform-module-for-aws-vpc-creation-fa2e5d0f3e3e?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/fa2e5d0f3e3e</guid>
            <category><![CDATA[terraform-modules]]></category>
            <category><![CDATA[aws-vpc]]></category>
            <category><![CDATA[terraform]]></category>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[aws]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Wed, 23 Jun 2021 09:13:33 GMT</pubDate>
            <atom:updated>2021-06-23T09:13:33.365Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uyj-t8HBumEnRQ9yNNdUrQ.jpeg" /></figure><p>Creating VPC involves a lot of steps like creating the VPC first and then creating a subnet and then routing tables and gateways. So I decided to create a module to make this process easy.</p><p>Here is the the terraform registry link: <br><a href="https://registry.terraform.io/modules/smc181002/vpc/aws/latest">smc181002/vpc/aws | Terraform Registry</a></p><h3>Creating a Terraform VPC launcing module</h3><p>First, we need to create a directory with a minimal file tree being:</p><pre>.<br>├─ LICENSE<br>├─ main.tf<br>├─ outputs.tf<br>├─ README.md<br>├─ variables.tf</pre><p>I have used the MIT LICENSE for this module. There are a lot other OpenSource Licenses available like the Apache 2.0.</p><p>Now lets start with the variables.tf file where the inputs for the module are defined.</p><pre>variable &quot;vpc_name&quot; {<br>  type    = string<br>  default = null<br>}</pre><pre>variable &quot;vpc_cidr_block&quot; {<br>  type    = string</pre><pre>validation {<br>    condition = can(regex(&quot;^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\/([0-9]|[1-2][0-9]|3[0-2]))?$&quot;, var.vpc_cidr_block))</pre><pre>error_message = &quot;The VPC cidr block is an invalid network name. Choose a valid network network name.&quot;<br>  }<br>}</pre><pre>variable &quot;subnet_names&quot; {<br>  type    = list(any)<br>  default = null<br>}</pre><pre>variable &quot;subnet_cidr_block&quot; {<br>  type    = list(any)</pre><pre>validation {<br>    condition     = can([for cidr in var.subnet_cidr_block : regex(&quot;^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\/([0-9]|[1-2][0-9]|3[0-2]))?$&quot;, cidr)])<br>    error_message = &quot;The Subnet cidr block is an invalid network name. Choose a valid network network name.&quot;<br>  }<br>}</pre><pre>variable &quot;gateway_name&quot; {<br>  type    = string<br>  default = null<br>}</pre><pre>variable &quot;route_table_name&quot; {<br>  type    = string<br>  default = null<br>}</pre><p>There are 2 required values (the vpc cidr and subnet cidr blocks). I have decieded to validate the input with regex which checks for the the network name of the format 10.0.0.0/16 and it rejects invalid network names like 10.0.0.256/24 and 10.0.0.0/33</p><p>There are 3 more variables which are optional. Visit the <a href="https://registry.terraform.io/modules/smc181002/vpc/aws/latest">smc181002/vpc/aws | Terraform Registry</a> page to check the documentation.</p><p>Now coming to the main.tf file, the code is given below</p><pre>resource &quot;aws_vpc&quot; &quot;mod_vpc&quot; {<br>  cidr_block = var.vpc_cidr_block</pre><pre>tags = {<br>    Name = var.vpc_name<br>  }<br>}</pre><pre>resource &quot;aws_subnet&quot; &quot;mod_subnet&quot; {<br>  depends_on = [<br>    aws_vpc.mod_vpc<br>  ]</pre><pre>count      = length(var.subnet_cidr_block)<br>  vpc_id     = aws_vpc.mod_vpc.id<br>  cidr_block = element(var.subnet_cidr_block, count.index)</pre><pre>tags = {<br>    Name = element(var.subnet_names, count.index)<br>  }<br>}</pre><pre>resource &quot;aws_internet_gateway&quot; &quot;mod_igw&quot; {<br>  depends_on = [<br>    aws_vpc.mod_vpc<br>  ]<br>  vpc_id = aws_vpc.mod_vpc.id</pre><pre>tags = {<br>    Name = var.gateway_name<br>  }<br>}</pre><pre>resource &quot;aws_route_table&quot; &quot;mod_route_table&quot; {<br>  depends_on = [<br>    aws_vpc.mod_vpc<br>  ]</pre><pre>vpc_id = aws_vpc.mod_vpc.id</pre><pre>route {<br>    cidr_block = &quot;0.0.0.0/0&quot;<br>    gateway_id = aws_internet_gateway.mod_igw.id<br>  }</pre><pre>tags = {<br>    Name = var.route_table_name<br>  }</pre><pre>}</pre><pre>resource &quot;aws_route_table_association&quot; &quot;mod_route_association&quot; {<br>  depends_on = [<br>    aws_vpc.mod_vpc,<br>    aws_subnet.mod_subnet<br>  ]</pre><pre>count          = length(aws_subnet.mod_subnet)<br>  subnet_id      = aws_subnet.mod_subnet[count.index].id<br>  route_table_id = aws_route_table.mod_route_table.id<br>}</pre><p>The Above code is just a basic VPC creation with public route tables being associated to the subnets.</p><p>Now coming to the outputs.tf, we have</p><pre>output &quot;vpc&quot; {<br>  value = aws_vpc.mod_vpc<br>}</pre><pre>output &quot;subnet&quot; {<br>  value = aws_subnet.mod_subnet<br>}</pre><pre>output &quot;gateway&quot; {<br>  value = aws_internet_gateway.mod_igw<br>}</pre><p>Here the VPC, Subnet and the gateway information are outputted. Wee have the subnet as a list in this case because multiple subnets can be launched in the instance.</p><h3>Publishing module to Terraform registry</h3><p>There are certain guidelines for creating a terraform registry mentioned in the documentation <a href="https://www.terraform.io/docs/registry/modules/publish.html#requirements">Terraform Registry — Publishing Modules — Terraform by HashiCorp</a>. But I will be giving a gist of what to do here.</p><ul><li>First, we need to create a GitHub repo. There is a convention for the naming of the GitHub repo. <br>The name should be of the format terraform-&lt;PROVIDER&gt;-&lt;NAME&gt;. In this article, I have created an AWS provider based module for VPC operations. So I named it terraform-aws-vpc. <a href="https://github.com/smc181002/terraform-aws-vpc">smc181002/terraform-aws-vpc (github.com)</a></li><li>Then we need to upload our code to the repository.</li><li>After uploading to the registry, we need to create a release in GitHub with a tag for the release which follows <a href="https://semver.org/">semantic versioning</a>. Some examples of semantic versioning are v1.0 or v2.3.4 or v0.2-alpha or v5.9-beta.3 .</li><li>Now all we need to do is to go to the <a href="https://registry.terraform.io/">https://registry.terraform.io/</a> and we can publish from GitHub from <a href="https://registry.terraform.io/github/create">this link</a>.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zl_Y2BWLVNztNWxrUlnT5w.png" /></figure><ul><li>There is an option at the top right to publish a module and the terraform will search for the GitHub repos in the account which follows the above requirements</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*F37OQyqjPcxIKm3QWM2E3g.png" /></figure><ul><li>Once when we click on the repo we want to publish, it will be automatically published to the registry.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XWRgHQtigXT2EGI_sOfRXA.png" /></figure><p>🎉 we now have published a new module to the terraform registry.<br>Here is an <a href="https://registry.terraform.io/modules/smc181002/vpc/aws/latest">example code</a> that you can try for launching your own VPC.</p><p>If you liked the article, Please applaud for this story and drop a star in the <a href="https://github.com/smc181002/terraform-aws-vpc">GitHub repository</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fa2e5d0f3e3e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Managed MySQL with Amazon RDS]]></title>
            <link>https://smc181002.medium.com/managed-mysql-with-amazon-rds-a56f7fab8b55?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/a56f7fab8b55</guid>
            <category><![CDATA[amazon-rds]]></category>
            <category><![CDATA[wordpress]]></category>
            <category><![CDATA[mysql]]></category>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[righteducation]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Mon, 24 May 2021 06:53:34 GMT</pubDate>
            <atom:updated>2021-05-24T06:53:34.679Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SWUFFHefud5dSSlwJteN9A.jpeg" /></figure><p>In this post, I am going to show how to launch a managed MySQL server with the help of Amazon RDS service.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/915/1*dGmRbH8wIplg031Tn33HVw.png" /></figure><p>First, we can choose how to create the Database. I have chosen the Standard create where Ican configure options manually whereas the easy create will do those jobs for us</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/885/1*pj2H0flcRjs01SI2cnDyQA.png" /></figure><p>Now we need to choose which database engine we should choose for our Relational database. I have chosen the MySQL DB version 5.7 so that it will work with my WordPress which is going to act as frontend which will connect to our database to store the information</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/907/1*hCSRcLrdcIeaN8FFK15gIg.png" /></figure><p>After choosing the DB engine, we need to choose a template depending upon your use case. A production template provides high availability and fast performance and a Dev/Test template can be used for the developer to run their applications which will be tested to isolate this from the production environment and Free tier to check the features of the RDS</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/880/1*31Rv2y97JmkdFhGfCUBegQ.png" /></figure><p>Now we need to setup the details of a master user and password and a unique name for the DB instance as an identifier</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/892/1*zemQHmQWQ5qO619Dfw08oA.png" /></figure><p>Now we can choose our Instances which we want to use. There are different instance classes for different use cases and a class specialized for DB instances. But for now i am using the t2.micro</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/877/1*TidrS3Q9rCh-xRH6qko62Q.png" /></figure><p>Now we can choose our storage and since i have chosen the GP ssd, user gets option to have a max of 16,384GiB. Also we can set our own threshold storage to limit the auto scaling feature.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/722/1*2HNsaU8RRGos7roG9SYpwQ.png" /></figure><p>Now either we can create a separate VPC or use an existing VPC. I chose to use my default VPC to launch the DB And denied the public access to only access from the ec2 instances in that VPC with a security group and availability zone of my choice</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/990/1*It0i9_1raKlaplcK3apX7Q.png" /></figure><p>We can create a DB inside this DB cluster we are going to create which will be used by the wordpress application to store the information.</p><p>Now we can configure the wordpress instance.</p><p>We can launch a normal Amazon Linux instance and download the wordpress</p><pre># php software for wordpress to work<br>yum install php php-mysqlnd php-fpm httpd php-json -y</pre><pre>#install wordpress software</pre><pre>curl https://wordpress.org/latest.tar.gz<br>tar xzf latest.tar.gz<br>cp -r wordpress/* /var/www/html/</pre><pre># selinux permissions for the wordpress directory<br><br>chown -R apache:apache /var/www/html/wordpress<br>chcon -t httpd_sys_rw_content_t /var/www/html/wordpress -R</pre><pre># restart the apache server<br>systemctl restart httpd<br>systemctl enable httpd</pre><p>After running the above commands in the same order, now user can open the wordpress server with the IP of the ec2 instance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*M0AKCrMKviJQXgYWR1xHVg.png" /></figure><p>Now if this page comes up, it means your wordpress software installation is working and now we need to connect the database to the wordpress. we can do it directly in the wp-config.php file inside the wordpress or use the web UI to do it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HRFQbVnVWQwvxbLm5Nxr2g.png" /></figure><p>Here we enter the database information like username, password, database name and the endpoint of the database and submit the information</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/877/1*1songr_i4u2A22vFEutf-Q.png" /></figure><p>Now we can create a new site by creating a new user and setting the site and install the wordpress site with the configured backend information and we now will be directed to our dashboard where we can edit our site and this site will be now public.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4FxTsLp7Mz7P48Fs51slVA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fjM0RZjjbZVVQPHsbtIk3g.png" /></figure><p>Thank you and I hope you have enjoyed this article and found it useful 😃</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a56f7fab8b55" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launching Kubernetes with Ansible roles]]></title>
            <link>https://smc181002.medium.com/launching-kubernetes-with-ansible-roles-5f53b91d8f20?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/5f53b91d8f20</guid>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[ansible]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[aws]]></category>
            <category><![CDATA[ansible-collection]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Sat, 10 Apr 2021 16:24:21 GMT</pubDate>
            <atom:updated>2021-04-10T16:24:21.950Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*k-AiCBcEgVz9r_mjcUZ2AA.jpeg" /></figure><p>Launching Kubernetes is a tedious task and this problem can be solved if we can automate the task with configuration tools like ansible.</p><p>So I created an ansible collection which consists roles for provisioning the instances and then configuring the cluster. Now below is how I coded the roles</p><h4>Provisioning role</h4><pre>- name: install boto<br>  pip:<br>    name: boto</pre><pre>- name: create kube master<br>  ec2:<br>    key_name: &quot;{{ key_name }}&quot;<br>    group: &quot;{{ sec_grp }}&quot;<br>    instance_type: &quot;{{ os_type }}&quot;<br>    image: &quot;{{ ami_id }}&quot;<br>    wait: true<br>    region: ap-south-1<br>    exact_count: 1<br>    count_tag:<br>      Name: kube_master<br>    instance_tags:<br>      Name: kube_master<br>      App: kube<br>  register: kube_ec2_master</pre><pre>- name: create kube slaves<br>  ec2:<br>    key_name: &quot;{{ key_name }}&quot;<br>    group: &quot;{{ sec_grp }}&quot;<br>    instance_type: &quot;{{ os_type }}&quot;<br>    image: &quot;{{ ami_id }}&quot;<br>    wait: true<br>    region: ap-south-1<br>    exact_count: 2<br>    count_tag:<br>      Name: kube_slave<br>    instance_tags:<br>      Name: kube_slave<br>      App: kube<br>  register: kube_ec2_slave</pre><ol><li>First task is to install boto library in the machine which we are using to provision the other instances (I executed this inside localhost).</li><li>Then we need to create the Kubernetes Master with required tags and of count 1.</li><li>Next, we crate Kubernetes Slave with count as 2 and the other variables will be passed as vars</li></ol><p>Vars file</p><pre>---<br># vars file for provision</pre><pre>key_name: mypem11<br>sec_grp: ssh-only<br>ami_id: ami-0a9d27a9f4f5c0efc<br>os_type: t2.micro</pre><p>Here I have used amazon Linux but using RedHat will be a problem because docker isn&#39;t available in rhel8.</p><p>Since the provisioning is completed, now we go to configuring all the Kubernetes nodes and from the collection, there is a role named common</p><h4>Common configuration</h4><pre>- name: install container engine [docker] and iproute-tc<br>  package:<br>    name:<br>      - docker<br>      - iproute-tc<br>    state: present</pre><pre>- name: start the container engine<br>  service:<br>    name: docker<br>    state: started<br>    enabled: yes</pre><pre>- name: Copy kubernetes repo file<br>  copy:<br>    src: kubernetes.repo<br>    dest: /etc/yum.repos.d/</pre><pre>- name: Install kubectl, kubelet, kubeadm programs<br>  yum:<br>    name:<br>      - kubectl<br>      - kubelet<br>      - kubeadm<br>    state: present<br>    disable_excludes: kubernetes</pre><pre>- name: change the cgroup driver to systemd<br>  copy:<br>    src: daemon.json<br>    dest: /etc/docker/<br>  notify: restart docker</pre><pre>- name: enable bridging in iptables<br>  sysctl:<br>    name: net.bridge.bridge-nf-call-iptables<br>    value: &#39;1&#39;</pre><pre>- name: enable bridging in ip6tables<br>  sysctl:<br>    name: net.bridge.bridge-nf-call-ip6tables<br>    value: &#39;1&#39;</pre><ul><li>Here, first the packages of docker and iproute-tc (used for ip table config) are installed and then the docker service is started and enabled</li><li>Now the Kubernetes repo file is copied from the controller node to the managed node</li></ul><pre>[kubernetes]<br>name=Kubernetes<br>baseurl=<a href="https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch">https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch</a><br>enabled=1<br>gpgcheck=1<br>repo_gpgcheck=1<br>gpgkey=<a href="https://packages.cloud.google.com/yum/doc/yum-key.gpg">https://packages.cloud.google.com/yum/doc/yum-key.gpg</a> <a href="https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg">https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</a><br>exclude=kubelet kubeadm kubectl</pre><ul><li>Now we install the Kubernetes related software from the configured repo in before step and also disabling the excluded repos.</li><li>Now we need to change the cgroup driver to systemd and then we to restart the docker file if there is a change with handler notification</li></ul><pre>- name: restart docker<br>  service:<br>    name: docker<br>    state: restarted</pre><ul><li>Now we need to enable the bridging in iptables of ipv4 and ipv6</li></ul><p>Now we need to configure the master node</p><h4>Master Node config</h4><pre>- name: init kubectl<br>  command: &quot;kubeadm init --pod-network-cidr={{ net_name }} --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem&quot;<br>  ignore_errors: yes</pre><pre>- name: create .kube dir<br>  file:<br>    path: $HOME/.kube<br>    state: directory<br>    mode: 0755</pre><pre>- name: copy config<br>  command: cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</pre><pre>- name: change ownership<br>  shell: chown $(id -u):$(id -g) $HOME/.kube/config</pre><pre>- name: create token for connecting workernodes<br>  command: &#39;kubeadm token create --print-join-command&#39;<br>  register: jointoken</pre><pre>- name: create flannel overlay network<br>  shell: &quot;curl <a href="https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml">https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</a> | sed &#39;s@10.244.0.0/16@{{net_name}}@&#39; &gt; kube-flannel.yml; kubectl apply -f kube-flannel.yml&quot;</pre><ul><li>First stem is to initialize the kubernetes cluster with kubeadm init command and the pod network is provided from the user input</li><li>After cluster creation, we need to copy the config file to access the endpoint of Kubernetes to the $HOME/.kube directory</li><li>After this, we need to create a token that can be used for the worker nodes to join the cluster.</li><li>Then we create a overlay network with the help of flannel. but we need to change the default network name to the user’s network name specified and we can achieve this with sed command.</li></ul><p>Now in slave node all we need to do is to get the command output from the master node for the token so that the slave node can connect to the master</p><h4>Slave Node configuration</h4><pre>- name: run the join command<br>  shell: &quot;{{join_command}}&quot;<br>  ignore_errors: yes</pre><p>Now we need to create a playbook to make this work</p><p>First we need to configure our dynamic inventory that I have already discussed in my previous articles.</p><pre>plugin: amazon.aws.aws_ec2<br>regions:<br>  - ap-south-1<br>keyed_groups:<br>  - key: tags<br>    prefix: tag<br>hostnames:<br>  - ip-address</pre><p>After we have the dynamic inventory, we need to configure our ansible.cfg file which is as below</p><pre>[defaults]<br>inventory=$HOME/inventory/aws_ec2.yml<br>host_key_checking=False</pre><h4>provision.yml for provisioning the instances</h4><pre>- hosts: localhost<br>  collections:<br>    - smc181002.k8s<br>  tasks:<br>    - name: launch nodes<br>      include_role:<br>        name: provision<br>      vars:<br>        key_name: mypem11<br>        sec_grp: allow-all<br>        ami_id: ami-0bcf5425cdc1d8a85<br>        os_type: t2.micro</pre><pre>- set_fact:<br>        instances: &quot;{{ kube_ec2_master.instances + kube_ec2_slave.instances }}&quot;</pre><pre>- name: wait for ssh to start<br>      wait_for:<br>        host: &quot;{{item.public_dns_name}}&quot;<br>        port: 22<br>        state: started<br>      loop: &quot;{{instances}}&quot;</pre><p>Now that we provisioned the instances, we need to create the playbook to configure the nodes with the collection roles</p><h4>main.yml for configuration</h4><pre>- hosts: tag_App_kube<br>  remote_user: ec2-user<br>  become: yes<br>  become_user: root<br>  collections:<br>    - smc181002.k8s<br>  roles:<br>    - common</pre><pre>- hosts: tag_Name_kube_master<br>  remote_user: ec2-user<br>  become: yes<br>  become_user: root<br>  collections:<br>    - smc181002.k8s<br>  tasks:<br>    - name: run master role<br>      include_role:<br>        name: master<br>      vars:<br>        net_name: &quot;10.240.0.0/16&quot;</pre><pre>- set_fact:<br>        join_command: &quot;{{ jointoken }}&quot;<br>    - debug:<br>        msg: &quot;{{hostvars[groups[&#39;tag_Name_kube_master&#39;][0]][&#39;join_command&#39;][&#39;stdout&#39;]}}&quot;</pre><pre>- hosts: tag_Name_kube_slave<br>  remote_user: ec2-user<br>  become: yes<br>  become_user: root<br>  collections:<br>    - smc181002.k8s<br>  tasks:<br>    - name: slave config<br>      include_role:<br>        name: slave<br>      vars:<br>        join_command: &quot;{{hostvars[groups[&#39;tag_Name_kube_master&#39;][0]][&#39;join_command&#39;][&#39;stdout&#39;]}}&quot;</pre><ul><li>The tag_App_kube is the group which includes all the nodes of the cluster which have a common configuration and <strong>common</strong> role from smc181002.k8s is used</li><li>Next comes the master node configuration and we use the master role from smc181002.k8s and then we save the token command output to host vars by setting with set_facts module</li><li>Then we configure the slaves by passing the command that the nodes should run to join the kubernetes cluster from the host vars that is set in the master node config role.</li><li>And after this, we can run this by first adding the ssh key for the instances so that ansible can contact the instances in AWS.</li></ul><pre>eval `ssh-agent`;<br>ssh-add /path/to/pem_file</pre><ul><li>After running the playbook commands, we can now login to the master node and use kubectl commands from the root user just like we do in minikube.</li></ul><p>Hope you enjoyed the article and found it useful</p><p><a href="https://galaxy.ansible.com/smc181002/k8s">Ansible Galaxy</a> — ansible collection link</p><p><a href="https://github.com/smc181002/k8s/">https://github.com/smc181002/k8s/</a> — github code</p><p><a href="
https://github.com/smc181002/create_kube_cluster">smc181002/create_kube_cluster (github.com)</a> — includes the playbook files too</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5f53b91d8f20" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Configuring docker containers with ansible via SSH]]></title>
            <link>https://smc181002.medium.com/configuring-docker-containers-with-ansible-via-ssh-b7eb5b672b27?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/b7eb5b672b27</guid>
            <category><![CDATA[docker]]></category>
            <category><![CDATA[apache-httpd]]></category>
            <category><![CDATA[ssh]]></category>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[ansible]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Sun, 28 Mar 2021 15:42:07 GMT</pubDate>
            <atom:updated>2021-03-28T15:42:07.407Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wvqg-GG4w2LhAAmxQ-7CWA.jpeg" /></figure><p>In this blog, I am going to use ansible to configure docker container with httpd server. Now using ssh in docker is unadvisable in general case but since ansible works on SSH protocol.</p><p>Now I created a container image called <a href="https://hub.docker.com/r/smc181002/centos-ssh">centos-ssh</a>. I used a Key based authentication into the container image and here is the <a href="https://drive.google.com/file/d/1mz6NPm1lx1vTiPSqJmAyyvBU5br90Ty7/view?usp=sharing">private key</a>.</p><p>Since we have the container that has SSH server in it, now it is time to create the container and configure it with the help of ansible playbook</p><p>I have written a playbook that can launch a container and then retrive the IP from the JSON data and create an ansible inventory group with the help of add_host module in ansible.</p><p><strong>mail.yaml</strong></p><pre>- hosts: localhost<br>  vars_files:<br>    - ./vars.yml<br>  tasks:<br>    - name: install docker.py for python client of docker<br>      pip:<br>        name: docker</pre><pre>- name: start docker<br>      service:<br>        name: docker<br>        state: started<br>        enabled: yes</pre><pre>- name: pulling the image with ssh pre-configured<br>      docker_image:<br>        name: smc181002/centos-ssh:latest<br>        source: pull</pre><pre>- name: launch a container<br>      docker_container:<br>        name: &quot;{{cname}}&quot;<br>        image: smc181002/centos-ssh:latest<br>        labels:<br>          type: webserver</pre><pre>- name: get all the runnning containers with webserver label<br>      docker_host_info:<br>        containers: yes<br>        verbose_output: yes<br>        containers_filters:<br>          label: &quot;type=webserver&quot;<br>          status: running<br>      register: running_container</pre><pre>- name: add the containers to a group<br>      add_host:<br>        name: &quot;{{item.NetworkSettings.Networks.bridge.IPAddress}}&quot;<br>        groups: webserver_containers<br>      loop: &quot;{{running_container.containers}}&quot;</pre><pre>- name:<br>      wait_for:<br>        port: 22</pre><pre>- hosts: webserver_containers<br>  tasks:<br>    - name: &quot;Install the httpd package&quot;<br>      package:<br>        name: httpd<br>        state: present<br>    - name: copy the webserver files from localhost to the servers<br>      copy:<br>        src: ./pages/index.html<br>        dest: /var/www/html/<br>    - name: &quot;start the httpd service&quot;<br>      shell: /usr/sbin/httpd</pre><p><strong>vars.yml</strong></p><pre>cname: webserver</pre><ul><li>In the above code, as you can see, I have first installed the docker.py package that helps us operate the docker containers from ansible playbook.</li><li>After that I started the docker service to configure the containers and then pulled the image that I have mentioned earlier</li><li>Then I created a container and added label to the container so that we can filter the containers that exist so that we can only add certain containers to our container list and then saved the output to running_container variable.</li><li>Now I need to add the container IP to the ansible inventory and this can be done with the help of add_host module from ansible</li><li>Since it may take some time to launch the ssh daemon in the container, I used wait_for to wait until the application on port 22 (ssh server) starts.</li><li>Now the container creation is done and it is time to configure the container with httpd server. The configuration is almost similar to my configurations that I have done in my previous blogs but since there is no systemctl in containers, we need to directly run the application and hence I used shell command to start the httpd program.</li></ul><p>Below is the output of playbook execution</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rjvsSfF1CZ6sccRiJ4U0_A.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3PMu7Pz2ddy0AECUIsqXRw.png" /></figure><p>And below is the curl response to the IP of the container</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kmef6gbHuPJBS7qzWPtLow.png" /></figure><p>Thank you and hope you have enjoyed the article and found it useful. 😃</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b7eb5b672b27" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dynamically changing variables based on OS distro in Ansible Playbook]]></title>
            <link>https://smc181002.medium.com/dynamically-changing-variables-based-on-os-distro-in-ansible-playbook-6338cb4793ce?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/6338cb4793ce</guid>
            <category><![CDATA[ansible]]></category>
            <category><![CDATA[ansible-playbook]]></category>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[distro-linux]]></category>
            <category><![CDATA[linuxworld]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Sun, 28 Mar 2021 10:27:46 GMT</pubDate>
            <atom:updated>2021-03-28T10:27:46.618Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tGyDQ0BDzBmJ4WPtrhT41w.jpeg" /></figure><p>When it comes to some applications like webserver application from apache which has package name as httpd in RedHat based distros and apache2 in Debian based distros. Ansible cannot be intelligent in this case. but with the help of variables and facts, we can make ansible change the names depending on the distro or os_family of the managed nodes.</p><p>So, now most of the distros come into the family of <strong>RedHat</strong> or <strong>Debian</strong>. So I have created two files to store variables for redhat and debian based distros named as redhat.yaml and debian.yaml</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1012/1*8tppJC-R_XSQraUbe1occQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TouiIFTQhxdvIaC4P4f-wg.png" /></figure><p>And I have an index.html file in the pages directory which is in the same directory as the main.yaml file. Below is my sample html code.</p><pre>&lt;!DOCTYPE html&gt;<br>&lt;html&gt;<br>  &lt;head&gt;<br>      &lt;title&gt;Another simple example&lt;/title&gt;<br>  &lt;/head&gt;<br>  &lt;style&gt;<br>    <a href="http://twitter.com/import">@import</a> url(&#39;<a href="https://fonts.googleapis.com/css2?family=Raleway:wght@800&amp;display=swap&#39;">https://fonts.googleapis.com/css2?family=Raleway:wght@800&amp;display=swap&#39;</a>);<br>    body {<br>      display: flex;<br>      margin: 0;<br>      justify-content: center;<br>      align-items: center;<br>    }</pre><pre>h1 {<br>      text-align: center;<br>      font-size: 32px;<br>      font-family: &quot;Raleway&quot;, sans-serif;<br>    }<br>  &lt;/style&gt;<br>  &lt;body&gt;<br>      &lt;h1&gt;Hello there!&lt;/h1&gt;<br>  &lt;/body&gt;<br>&lt;/html&gt;</pre><p>Now we can get into the main.yaml file. we need to</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ppPXuZOKNSoHaPCR9_u7lw.png" /></figure><pre>- hosts: webservers<br>  vars:<br>    - pkg_names:<br>        files:<br>          - &quot;{{ansible_os_family | lower }}.yaml&quot;<br>  tasks:<br>    - name: Load var file depending on the os family<br>      include_vars: &quot;{{ lookup(&#39;first_found&#39;, pkg_names) }}&quot;</pre><pre>- name: &quot;Install the {{pkg_name}} package&quot;<br>      package:<br>        name: &quot;{{pkg_name}}&quot;<br>        state: present</pre><pre>- name: copy the webserver files from localhost to the servers<br>      copy:<br>        src: ./pages/index.html<br>        dest: /var/www/html/<br>      notify:<br>        - restart httpd</pre><pre>- name: &quot;start the {{pkg_name}} service&quot;<br>      service:<br>        name: &quot;{{pkg_name}}&quot;<br>        state: started<br>  handlers:<br>    - name: restart httpd<br>      service:<br>        name: &quot;{{pkg_name}}&quot;<br>        state: restarted</pre><ul><li>Here in the above code, first i created a variables of the file names depending on the distros being used.</li><li>Then I created a task to include the variable depending up on the os distro</li><li>Now the installation and setup is same as the normal setup of httpd in the RedHat that I did in the previous blogs.</li><li>I also added a handler that restarts the apache webserver when the files inside the server have changed</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WpJCU_ul7B2TBtkPmv_BVA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eA9tDlRQnOzy1Qj-h7QOBQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OdbZHahp4Dmor5TKp1eRkg.png" /></figure><p>As you can see, the same configuration is done to both of the instances even though the names of the packages are different.</p><p>Hope you liked this article and found it useful. 😃</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6338cb4793ce" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating a Network Topology such that A can connect to B and C but B and C can only connect to A]]></title>
            <link>https://smc181002.medium.com/creating-a-network-topology-such-that-a-can-connect-to-b-and-c-but-b-and-c-can-only-connect-to-a-28c64589be3f?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/28c64589be3f</guid>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[vimal-daga]]></category>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[linuxworld]]></category>
            <category><![CDATA[network-topology]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Sun, 28 Mar 2021 06:17:35 GMT</pubDate>
            <atom:updated>2021-03-28T06:17:35.364Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*chDosGd2WxusZiQeJdGRyA.jpeg" /></figure><p>Here we have 3 VMs which have connectivity with each other at the start. But we want to configure it such that the <strong>mn1</strong> can ping to <strong>mn2</strong> and <strong>mn3</strong> but <strong>mn2</strong> and <strong>mn3</strong> can ping only to <strong>mn1</strong> and not to each other.</p><p>Below are the IPs for the ens224 adapter.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/735/1*KRWULz8P1qPZMBBWBaHxhQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/761/1*dZ_V01cXuevH4XZyO_leoQ.png" /></figure><p>Now first let us clean the route tables existing in the VMs previously. Doing this will make the VM completely isolated. Then we can add the required routes to the route tables of the nodes.</p><p>we can clean the route table with the below command</p><pre>ip route flush table main</pre><p>Now for the mn1, create a new route that will allow all the packets from IPs of network name 192.168.226.0/24 (netmask: 255.255.255.0) so that mn1 can ping mn2 and mn3</p><pre>route add -net 192.168.226.0 netmask 255.255.255.0 ens224</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/674/1*kBzMXa4XBW6k9v9LOxAvNw.png" /></figure><p>Now coming to the mn2 and mn3, we need to add only the IP of the mn1. so the route table rule will be 192.168.226.132/32 (netmask: 255.255.255.255)</p><pre>route add -net 192.168.226.132 netmask 255.255.255.255 ens224</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/651/1*KClLqDoEFuaJ3dfxzy5jaA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/661/1*8a068sYKlXb9sAHevX9Qcw.png" /></figure><p>Now we will test the results</p><p>MN1</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/536/1*Mtmdg9LBWoDOZ88d04xu1g.png" /></figure><p>MN2</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/527/1*2DkMeCc4647S539KkQHFxQ.png" /></figure><p>MN3</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/540/1*rvCMZ_KXMMly8KExNm5q4w.png" /></figure><p>From the results, we can say that mn2 and mn3 can ping to mn1 but not to each other and mn1 can ping to mn2 and mn3.</p><p>Hope you liked the article and found it useful. 😃</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=28c64589be3f" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Running GUI application in Docker with Xming]]></title>
            <link>https://smc181002.medium.com/running-gui-application-in-docker-with-xming-e9d81c5600e?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/e9d81c5600e</guid>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[linuxworld]]></category>
            <category><![CDATA[containers]]></category>
            <category><![CDATA[docker]]></category>
            <category><![CDATA[vimal-daga]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Tue, 23 Mar 2021 09:35:14 GMT</pubDate>
            <atom:updated>2021-03-23T09:35:14.257Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8a4JQ0vJtMCsum3u8Ld6Hg.jpeg" /></figure><p>In this article, I am going to attach the docker container GUI in the windows with the help of Xming app.</p><p>Download the application from this website <a href="https://sourceforge.net/projects/xming/">Xming X Server for Windows download | SourceForge.net</a></p><p>Xming is a X server for windows which helps us to display the output from the virtual machines, containers and from cli based OS when we want to run a GUI application</p><p>We can configure the settings using the command XLaunch app and choose the display settings</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/622/1*93W8VS9kCr4PItLsXZr4eg.png" /></figure><p>The Display number can be set as you wish and a new port will be open. We can have multiple display numbers to launch multiple instances of xming to connect multiple containers if we wish to</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*ZkDA1imaPR2Ydnmmi89DLA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/623/1*MRd6fAVIn8CRywPNhS8Y9w.png" /></figure><p>Then click final to save the configuration</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/822/1*ajgEF0H4d6N5ZH8WuVwERA.png" /></figure><p>If you want to do the process manually, you need to first create a centos container and type the following commands</p><pre>dnf install -y mesa-libGLU-devel gcc-c++ firefox</pre><pre>export DISPLAY=&lt;YOUR_IP&gt;:&lt;DISPLAY_NO&gt;</pre><pre>/usr/bin/firefox</pre><p>This is the docker file created. I have uploaded the image in the docker hub if you want to try the same.</p><pre>docker pull smc181002/gui_on_container</pre><p>To change the environment variable you can use the -e option when creating the container</p><pre>docker run -it --rm -e DISPLAY=192.168.0.102:5.0 smc181002/gui_on_container</pre><p>Now if your configuration was right, the window will open automatically</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sRjDJ27A5roqJhL3LXElqQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v8LJVnHxbtrwI05vUOPeTw.png" /></figure><p>As you can see, we can access the Firefox from windows which is actually executed in the docker container in my RedHat VM.</p><p>Hope you enjoyed the article and found it useful. 😃</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e9d81c5600e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenShift Industry Use Cases]]></title>
            <link>https://smc181002.medium.com/openshift-industry-use-cases-b20ca056de9c?source=rss-ee5d12b9cb1a------2</link>
            <guid isPermaLink="false">https://medium.com/p/b20ca056de9c</guid>
            <category><![CDATA[openshift]]></category>
            <category><![CDATA[devops]]></category>
            <category><![CDATA[linuxworld]]></category>
            <category><![CDATA[arthbylw]]></category>
            <category><![CDATA[vimal-daga]]></category>
            <dc:creator><![CDATA[Meherchaitanya]]></dc:creator>
            <pubDate>Sat, 13 Mar 2021 15:39:48 GMT</pubDate>
            <atom:updated>2021-03-13T15:39:48.158Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7ZO6HpUbwjGeA0SXPlwCsw.jpeg" /></figure><p>OpenShift is a Container platform tool that uses the Kubernetes and many other concepts like builds for continuous integration and continuous deployment and also provide monitoring for the application. With the help of OpenShift one can just provide the link of the GitHub or GitHub like cloud servers which store code and OpenShift will directly download the code and convert into a docker image and use it to create containers which are very efficient with resources compared to other methods</p><p>OpenShift benefits us as Developers, IT operators and Business Leaders</p><h3>OpenShift for developers</h3><p>Kubernetes is a service that is very much used because of it’s use of containers to launch applications in the most efficient with automatic failover relaunching of resources and also being very portable. But for developers, Kubernetes is hard to manage when we only want to just launch the apps. But with the help of OpenShift, we don&#39;t need to worry about the CI/CD pipelines and OpenShift itself manages it directly.</p><h3>OpenShift for IT Operations</h3><p>Scaling and complexity are common in applications these days. The load of the application may increase at any time and having less instances for the user to access will lead to loss in customers and if we have more than required, then there will be a loss in resources. Containers remove that problem because of the speed of scaling is very fast in containers because you can create a container just in seconds. with the help of containers and workflow automation, one can directly just focus on the code building and can easily deploy, test and build the code. OpenShift platform provides all the workflow automation and container orchestration as default without any external integrations and also provide monitoring for the application</p><h3>OpenShift for Business Leaders</h3><p>When it comes to the large organizations, Hybrid cloud computing is a famous norm which is the idea of utilizing cloud services like AWS, GCP, Azure and also create clouds on-premise like OpenStack different from OpenStack) to create private clouds and combine them to create efficient infrastructure and higher safety because some companies like banks cannot have their data in public clouds due to their company terms. OpenShift partnered with major cloud companies to create dedicated managed services in AWS and Azure. This allows us to have OpenShift in multiple clouds and also OpenShift software can be downloaded and installed on the private servers directly with the same features. This removes the problem of migrations and security issues because one can easily migrate from one cloud service to another without no problem of thinking to redesign the whole infrastructure in the other cloud and also one can install the software in their private servers for security terms.</p><p>Hope you liked the article and found it useful 😃.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b20ca056de9c" width="1" height="1" alt="">]]></content:encoded>
        </item>
    </channel>
</rss>
